
<html>
<head>
	<title> Welcome to the Homepage of Tanbir Ahmed! </title>
	<link rel="stylesheet" href="../style.css" type="text/css" />
        <link href='http://fonts.googleapis.com/css?family=Lobster+Two' rel='stylesheet' type='text/css'>
	 <link href='https://fonts.googleapis.com/css?family=Alegreya' rel='stylesheet'>
 	<link href='https://fonts.googleapis.com/css?family=Barlow Condensed' rel='stylesheet'>
  	<link href='https://fonts.googleapis.com/css?family=Roboto Condensed' rel='stylesheet'>
 	<link href='https://fonts.googleapis.com/css?family=Encode Sans Condensed' rel='stylesheet'>
        <link href='http://fonts.googleapis.com/css?family=Berkshire+Swash' rel='stylesheet' type='text/css'>
	<link href='https://fonts.googleapis.com/css?family=PT+Serif:400' rel='stylesheet' type='text/css'>
        <link href='https://fonts.googleapis.com/css?family=PT+Serif:400italic' rel=stylesheet' type='text/css'>
        <link href='http://fonts.googleapis.com/css?family=Merienda+One' rel='stylesheet' type='text/css'>
		<link href='http://fonts.googleapis.com/css?family=Simonetta' rel='stylesheet' type='text/css'>
        <meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
	<meta charset="UTF-8">
													      
	<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]}});</script>
													      
								
</head>

<body>
<table cellspacing=0 cellpadding=20 align=center><tr><td>													      
  <table cellspacing=0 cellpadding=0 align=center>
	<tr><td><H1>An implementation of a Feedforward Neural Network (FNN)</H1></td></tr>                                                                                                
	<tr><td><H2>Data Structure</H2></td></tr>												      
	<tr><td><H3>1. Layers and Parameters</H3></td></tr>                                                                                                
	<tr><td>
	<b>Layers:</b> Let $\ell_0, \ell_1, \ldots, \ell_L$ be the layers where <br>
	<ul>													      
		<li>	$\ell_0$ represents the input layer, 
    		<li>	$\ell_1, \ell_2, \ldots, \ell_{L-1}$ represent the hidden layers, and 
    		<li> 	$\ell_L$ represents the output layer. 
	</ul>												      
	Let $h_i$ denote the height of the layer $\ell_i$ for $i=0, 1, \ldots, L$ <br><br>													      
    	
	<b>Biases:</b> For $i=1, 2, \ldots, L$, let <br>
	<ul>
		<li> $b^{(i)} \in {\mathbb R}^{h_i}$ denote the bias for the layer $\ell_i$ and
		<li> $b^{(i)}_j \in {\mathbb R}$ denote the bias for the $j$-th neuron of $\ell_i$
	</ul>
	<b>Weights:</b> For $i=1, 2, \ldots, L$, let <br>
	<ul>
		<li> $w^{(i)} \in {\mathbb R}^{h_{i-1}\times h_{i}}$ denote the weight for the layer $\ell_i$ and
    		<li> $w^{(i)}_{jk}\in {\mathbb R}$ denote the weight from the $j$-th neuron of $\ell_{i-1}$ to the $k$-th neuron of $\ell_i$  													      
	</ul>
	<b>Layer outputs:</b> For $i=1, 2, \ldots, L$, $z^{(i)} \in {\mathbb R}^{h_i}$ for the layer $\ell_i$ be defined as follows:<br>    
    	<ul>
		<li>$z^{(1)} = \left(w^{(1)}\right)^T\cdot x + b^{(1)}$ where $x\in{\mathbb R}^{h_{i-1}}$ is the input vector
        	<li>$z^{(i)} = \left(w^{(i)}\right)^T\cdot a^{(i-1)} + b^{(i)}$ for $i=2,3,\ldots,L$ where $a^{(i)}$ is defined as below
	</ul>
	<b>Layer output activations:</b> For $i=0, 1, 2, \ldots, L$, let $a^{(i)}\in {\mathbb R}^{h_i}$ for the layer $\ell_i$ be defined as follows: <br>   
    	<ul>
		<li>$a^{(0)}\in {\mathbb R}^{h_0}$ is initialized to the input vector $x$ and  
        	<li>$a^{(i)}=\sigma(z^{(i)})$ for $i=1,2, \ldots,L$ with some activation function $\sigma$ (we discuss activation functions later)												      
	</ul>
	<b>Output of the network:</b> $a^{(L)} \in {\mathbb R}^{h_L}$ <br>												      
	</td></tr>												      
	<tr><td><H3>2. Computation of Errors</H3></td></tr>													      
	The most common optimization algorithms used in neural networks are different varients of Gradient Descent (GD) where the objective function is a cost function  CC  that we want to minimize.													      
  </table>
</td></tr></table>													     
</body>                                                                                                  
                                                                                                  
</html>
