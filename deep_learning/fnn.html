
<html>
<head>
	<title> Welcome to the Homepage of Tanbir Ahmed! </title>
	<link rel="stylesheet" href="../style.css" type="text/css" />
        <link href='http://fonts.googleapis.com/css?family=Lobster+Two' rel='stylesheet' type='text/css'>
	 <link href='https://fonts.googleapis.com/css?family=Alegreya' rel='stylesheet'>
 	<link href='https://fonts.googleapis.com/css?family=Barlow Condensed' rel='stylesheet'>
  	<link href='https://fonts.googleapis.com/css?family=Roboto Condensed' rel='stylesheet'>
 	<link href='https://fonts.googleapis.com/css?family=Encode Sans Condensed' rel='stylesheet'>
        <link href='http://fonts.googleapis.com/css?family=Berkshire+Swash' rel='stylesheet' type='text/css'>
	<link href='https://fonts.googleapis.com/css?family=PT+Serif:400' rel='stylesheet' type='text/css'>
        <link href='https://fonts.googleapis.com/css?family=PT+Serif:400italic' rel=stylesheet' type='text/css'>
        <link href='http://fonts.googleapis.com/css?family=Merienda+One' rel='stylesheet' type='text/css'>
		<link href='http://fonts.googleapis.com/css?family=Simonetta' rel='stylesheet' type='text/css'>
        <meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
	<meta charset="UTF-8">
													      
	<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]}});</script>
													      
								
</head>

<body>
<table cellspacing=0 cellpadding=20 align=center><tr><td>													      
  <table cellspacing=0 cellpadding=0 align=center>
	<tr><td><H1>An implementation of a Feedforward Neural Network (FNN)</H1></td></tr>                                                                                                
	<tr><td><H2>Data Structure</H2></td></tr>												      
	<tr><td><H3>1. Layers and Parameters</H3></td></tr>                                                                                                
	<tr><td>
	<b>Layers:</b> Let $\ell_0, \ell_1, \ldots, \ell_{L-1}$ be the layers where <br>
	<ul>													      
		<li>	$\ell_0$ represents the input layer, 
    		<li>	$\ell_1, \ell_2, \ldots, \ell_{L-2}$ represent the hidden layers, and 
    		<li> 	$\ell_{L-1}$ represents the output layer. 
	</ul>												      
	Let $h_i$ denote the height of the layer $\ell_i$ for $i=0, 1, \ldots, L-1$ <br><br>													      
    	
	<b>Biases:</b> For $i=1, 2, \ldots, L-1$, let <br>
	<ul>
		<li> $b^{(i)} \in {\mathbb R}^{h_i}$ denote the bias for the layer $\ell_i$ and
		<li> $b^{(i)}_j \in {\mathbb R}$ denote the bias for the $j$-th neuron of $\ell_i$
	</ul>
	<b>Weights:</b> For $i=1, 2, \ldots, L-1$, let <br>
	<ul>
		<li> $w^{(i)} \in {\mathbb R}^{h_{i-1}\times h_{i}}$ denote the weight for the layer $\ell_i$ and
    		<li> $w^{(i)}_{jk}\in {\mathbb R}$ denote the weight from the $j$-th neuron of $\ell_{i-1}$ to the $k$-th neuron of $\ell_i$  													      
	</ul>
	<b>Layer outputs:</b> For $i=1, 2, \ldots, L-1$, $z^{(i)} \in {\mathbb R}^{h_i}$ for the layer $\ell_i$ be defined as follows:<br>    
    	<ul>
		<li>$z^{(1)} = \left(w^{(1)}\right)^T\cdot x + b^{(1)}$ where $x\in{\mathbb R}^{h_{i-1}}$ is the input vector
        	<li>$z^{(i)} = \left(w^{(i)}\right)^T\cdot a^{(i-1)} + b^{(i)}$ for $i=2,3,\ldots,L-1$ where $a^{(i)}$ is defined as below
	</ul>
	<b>Layer output activations:</b> For $i=0, 1, 2, \ldots, L-1$, let $a^{(i)}\in {\mathbb R}^{h_i}$ for the layer $\ell_i$ be defined as follows: <br>   
    	<ul>
		<li>$a^{(0)}\in {\mathbb R}^{h_0}$ is initialized to the input vector $x$ and  
        	<li>$a^{(i)}=\sigma(z^{(i)})$ for $i=1,2, \ldots,L-1$ with some activation function $\sigma$ (we discuss activation functions later)												      
	</ul>
	<b>Output of the network:</b> $a^{(L-1)} \in {\mathbb R}^{h_{L-1}}$ <br><br>												      
	</td></tr>												      
	<tr><td><H3>2. Computation of Errors</H3></td></tr>													      
	<tr><td>												      
	The most common optimization algorithms used in neural networks are different varients of Gradient Descent (GD) where the objective function is a cost function  CC  that we want to minimize.<br><br>
	<b>Cost function:</b> Let $C$ be some differentiable cost function (we discuss cost functions later)<br><br>
	<b>Error in output:</b> For $j\in\{1, 2, \ldots, h_{L-1}\}$ 

	    $$\begin{eqnarray*}
	    \delta^{(L-1)}_j &=& {\partial C}/{\partial z^{(L-1)}_j} \\
		 &=& \sum_k\left({\partial C}/{\partial a^{(L-1)}_k}\right) \left({\partial a^{(L-1)}_k}/{\partial z^{(L-1)}_j}\right)  \\ 
		 &=& \left({\partial C}/{\partial a^{(L-1)}_j}\right) \left({\partial a^{(L-1)}_j}/{\partial z^{(L-1)}_j}\right) 
	    \end{eqnarray*}$$
    
    	Hence, error in the output layer:
	    $$\begin{eqnarray*}
	    \delta^{(L-1)} &=& 
		  \begin{bmatrix}
		   {\partial C}/{\partial z^{(L-1)}_1} \\
		   {\partial C}/{\partial z^{(L-1)}_2} \\
		    \cdots \\
		   {\partial C}/{\partial z^{(L-1)}_{h_L}} 
		   \end{bmatrix} \\
		  &=&
			 \begin{bmatrix}
			 {\partial C}/{\partial a^{(L-1)}_1} \\
			{\partial C}/{\partial a^{(L-1)}_2} \\
			\cdots \\
			{\partial C}/{\partial a^{(L-1)}_{h_L}}
			\end{bmatrix}
			\odot 
			\begin{bmatrix}
			{\partial a^{(L-1)}_1}/{\partial z^{(L-1)}_1} \\
			{\partial a^{(L-1)}_2}/{\partial z^{(L-1)}_2} \\
			\cdots \\
			{\partial a^{(L-1)}_{h_L}}/{\partial z^{(L-1)}_{h_L}}
			\end{bmatrix}     \\           
		&=&  \nabla_a{C}\odot \sigma^{\prime}(z^{(L-1)})
	    \end{eqnarray*}$$													      
	
	<b>Error propagated to other layers:</b> For $i=L-2, L-3, \ldots, 2, 1$,

	    $$\begin{eqnarray*}
	    \delta^{(i)}_j &=& {\partial C}/{\partial z^{(i)}_j} \\
		 &=& \sum_k\left({\partial C}/{\partial z^{(i+1)}_k}\right) \left({\partial z^{(i+1)}_k}/{\partial z^{(i)}_j}\right)  \\ 
		 &=& \left({\partial z^{(i+1)}_k}/{\partial z^{(i)}_j}\right) \delta^{(i+1)}_k 
	    \end{eqnarray*}$$
    
    		Since $z^{(i+1)} = \left(w^{(i+1)}\right)^Ta^{(i)}+b^{(i+1)}$, we have 
    
	    $$\begin{eqnarray}
		z^{(i+1)}_k &=& \sum_j\left(w^{(i+1)}\right)^T_{kj}a^{(i)}_j + b^{(i+1)}_k \\ 
		&=& \sum_jw^{(i+1)}_{jk}\sigma(z^{(i)}_j) + b^{(i+1)}_k     
		\end{eqnarray}$$
     
     		Therefore, ${\partial z^{(i+1)}_k}/{\partial z^{(i)}_j} = w^{(i+1)}_{jk}\sigma^\prime(z^{(i)}_j)$ which implies 
     		$$\delta^{(i)}_j = \sum_kw^{(i+1)}_{jk}\delta^{(i+1)}_k\sigma^\prime(z^{(i)}_j).$$ 
     
     	Hence,
    		$$\delta^{(i)} = \left(w^{(i+1)}\cdot \delta^{(i+1)}\right)\odot \sigma^{\prime}(z^{(i)}).$$													      
	<b>Rate of change of $C$ with respect to any bias:</b> 
	\begin{eqnarray}  
	  \frac{\partial C}{\partial b^{(i)}_j} &=& {\partial C}/{\partial b^{(i)}_j} = \delta^{(i)}_j.
	\end{eqnarray}

    Hence,
	\begin{eqnarray}  
	  \frac{\partial C}{\partial b^{(i)}} 
			&=& \begin{bmatrix}
			{\partial C}/{\partial b^{(i)}_1} \\
			{\partial C}/{\partial b^{(i)}_2} \\
			\cdots \\
			{\partial C}/{\partial b^{(i)}_{h_i}}
			\end{bmatrix}  \\
			&=& \begin{bmatrix}
			{\delta^{(i)}_1} \\
			{\delta^{(i)}_2} \\
			\cdots \\
			{\delta^{(i)}_{h_i}}
			\end{bmatrix} = \delta^{(i)}.
		\end{eqnarray}
<b>Rate of change of $C$ with respect to any weight:</b>
$$\begin{eqnarray}  
  \frac{\partial C}{\partial w^{(i)}_{jk}} =  \delta^{(i)}_ka^{(i-1)}_j.
\end{eqnarray}$$
													      
Further simplifying, $$\begin{eqnarray}  
  \frac{\partial C}{\partial w^{(i)}} =  \delta^{(i)}\left(a^{(i-1)}\right)^T.
\end{eqnarray}$$

Since above error-calculations are for a single input $x$. Let us extend our notation to incorporate the inputs. Let $\delta^{(x, i)}$ denote the error in layer $i$ on input $x$. Similarly, we extend the notation of $a{(i)}$ to $a^{(x,i)}$ for layer $i$.<br><br>
	</td></tr>											
													      
	<tr><td><H3>3. Optimization Algorithms</H3></td></tr>													      
	<tr><td><H4>3.1. Stochastic Gradient Descent</H4></td></tr>													      													      
	<tr><td>												      													      
	The standard <b>Batch Gradient Descent</b> algorithm uses $m$ to be equal to the size of the entire training dataset. 
	In this case, the errors are less noisy but the convergence is slow (since updating of parameters is less frequent) and 
	impractical if the dataset is big. The <b>Stochastic Gradient Descent (SGD)</b> algorithm uses $m$ to be 1 corresponding 
	to a single randomly picked example from the training dataset. In this case, the convergence is faster (since updating 
	of parameters is more frequent), but the error is noisier which in turn helps to escape shallow local minima more easily.  
	A more practical approach, known as <b>Mini-batch Gradient Descent</b>, is to combine the best of both the varients 
	and take $m$ to be equal to a small number greater than 1. An <b>epoch</b> is passing of all the training examples 
	through the neural network. It is important that the training dataset be shuffled at the beginning of each epoch for 
	randomness of the example(s) picked in both SGD and Mini-Batch algorithms. <br><br>

	Let the size of training dataset and a minibatch be respectively $M$ and $m$. Let $w_t^{(i)}$ and $b_t^{(i)}$ denote the weights $w^{(i)}$ and biases $b^{(i)}$ after update iteration $t$ with $t\geq 0$.  For $i=L-1, L-2, ..., 1$, the grdients are
	$$\begin{eqnarray}
	\frac{\partial C}{\partial w_t^{(i)}} &=&  {1\over m}\sum_x\delta^{(x, i)}\left(a^{(x, i-1)}\right)^T \\
	\frac{\partial C}{\partial b_t^{(i)}} &=&  {1\over m}\sum_x\delta^{(x, i)}\\
	\end{eqnarray}$$
	The weights are updated as follows:
	$$\begin{eqnarray}
	w_{t+1}^{(i)} &=& w_t^{(i)} - \eta \left[\frac{\partial C}{\partial w_t^{(i)}}\right] \\
	b_{t+1}^{(i)} &=& b_t^{(i)} - \eta \left[\frac{\partial C}{\partial b_t^{(i)}}\right]
	\end{eqnarray}$$

	where $\eta$ is the hyperparameter representing the learning-rate of the training. The learning rate $\eta$ (typically between $0.0$ and $1.0$) is perhaps the most important  hyperparameter to be tuned in a neural network. It determines the proportion of errors to be used to update the parameters of the model and controls the speed of learning. <br><br>
	</td></tr>
	<tr><td><H5>3.1.1 Scheduling the Learning Rate </H5></td></tr>
	<tr><td>
	Smaller learning rate requires more training epochs. Large learning rate may converge fast but only to a suboptimal solution. So it is a good idea to decay the learning rate as the algorithm moves from update to update. Let $\eta_t$ denotes the learning rate at the $t$-th update. The general rule of convergance is
	$\sum_{t=1}^\infty\eta_{t} = \infty$ and $\sum_{t=1}^\infty\eta_{t}^2 < \infty$. The learning rate may be decayed using some strategy. We use the following strategy:


	$$\eta_t = {\text{Initial learning rate} \over 1.0 + (\text{decay} * t)}$$

	For layer $i=L-1, L-2, ..., 2, 1$,


	$$\begin{eqnarray}
	w_{t+1}^{(i)} &=& w_t^{(i)} - \eta_t \left[\frac{\partial C}{\partial w_t^{(i)}}\right] \\
	b_{t+1}^{(i)} &=& b_t^{(i)} - \eta_t \left[\frac{\partial C}{\partial b_t^{(i)}}\right]
	\end{eqnarray}$$
	</td></tr>
	<tr><td><H5>3.1.2 Adding Momentum</H5></td></tr>
	<tr><td>
	Momentum in Physics means mass times velocity. In SGD, we consider unit mass, and so momentum adds speed and direction in the update procedure. To update the parameters, instead of only depending on the current gradient, we may depend on the exponentially weighted average of the current and past gradients. 
	For layer $i=L-1, L-2, ..., 1$, with $v_t^{(w,i)}$ and $v_t^{(b,i)}$ both initialized to zero, 

	\begin{eqnarray}
	v_t^{(w,i)} &=& \alpha v_{t-1}^{(w,i)} - \eta_t \left[\frac{\partial C}{\partial w_t^{(i)}}\right] \\
	v_t^{(b,i)} &=& \alpha v_{t-1}^{(b,i)} - \eta_t \left[\frac{\partial C}{\partial b_t^{(i)}}\right] \\
	\end{eqnarray}

	\begin{eqnarray}
	w_{t+1}^{(i)} &=& w_t^{(i)} + v_t^{(w,i)} \\
	b_{t+1}^{(i)} &=& b_t^{(i)} + v_t^{(b,i)}
	\end{eqnarray}

	Here $\alpha$ is the momentum hyperparameter. The larger the value of $\alpha$ than the learning rate $\eta$, the more previous gradients affect the current direction. Momentum accelerates the advancement in the direction of the gradient. A typical value of $\alpha$ is $0.9$. For $\alpha=0.9$, the speed is asymptotically 10 times faster than the standard SGD.<br><br>
	</td></tr>	
													      
	<tr><td><H5>3.1.3 Considering Lookahead (known as Nesterov Momentum)</H5></td></tr>
	<tr><td>
	\begin{eqnarray}
	w_{t+1}^{(i)} &=& w_t^{(i)} + \alpha v_{t}^{(w,i)} - \eta_t \left[\frac{\partial C}{\partial w_t^{(i)}}\right] \\
	b_{t+1}^{(i)} &=& b_t^{(i)} + \alpha v_{t}^{(b,i)} - \eta_t \left[\frac{\partial C}{\partial b_t^{(i)}}\right]
	\end{eqnarray}													      
	</td></tr>													      
													      
	<tr><td><H4>3.2 Adagrad (Adaptive Gradient) algorithm</H5></td></tr>
	<tr><td>
		In this algorithm, the learning rate is updated by dividing it by the square-root of the cumulative sum of current and past squared gradients. The learning rate of a parameter decreases faster if the partial derivative of the cost (loss) with respect to that parameter is larger. 
													      Excess decrease of learning rate may occur due to cumulative sum of gradients. <br><br>
	</td></tr>	
	<tr><td><H5>References</H5></td></tr> 	
	<tr><td>												      
		J. Duchi, E. Hazan, Y. Singer, <a href=http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf>Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a>, JMLR, <b>12</b> (2011) 2121-2159<br><br>
	</td></tr>	
	<tr><td><H5>3.2.1 Computing sum of squares of historic gradients</H5></td></tr>
	<tr><td>												      
	On update $t$ (with $t\geq 1$), for layer $i=L-1, L-2, ..., 1$, 
	\begin{eqnarray}
	s_t^{(w, i)} &=& s_{t-1}^{(w,i)} + \left[\frac{\partial C}{\partial w_t^{(i)}}\right]^2 \\
	s_t^{(w, i)} &=& s_{t-1}^{(b,i)} + \left[\frac{\partial C}{\partial b_t^{(i)}}\right]^2
	\end{eqnarray}

	with $s_0^{(w,i)}$ and $s_0^{(b,i)}$ both initialized to zero-vectors.<br><br>
	</td></tr>
	<tr><td><H5>3.2.2 Updating parameters</H5></td></tr>
	<tr><td>
	\begin{eqnarray}
	w_{t+1}^{(i)} &=& w_t^{(i)} - \frac{\eta_t}{\sqrt{s_t^{(w,i)} + \epsilon}}\left[\frac{\partial C}{\partial w_t^{(i)}}\right]  \\
	b_{t+1}^{(i)} &=& b_t^{(i)} - \frac{\eta_t}{\sqrt{s_t^{(b,i)} + \epsilon}}\left[\frac{\partial C}{\partial b_t^{(i)}}\right]
	\end{eqnarray}

	Adagrad would converge rapidly on a convex function. On a non-convex function it may eventually arrive at a convex region unless the learning rate became too small before that happens. The $\epsilon$ in the denominator is to avoid divide-by-zero errors. Keras uses $\eta=0.01$ and $\epsilon=10^{-7}$. <br><br>
	</td></tr>												      
      	<tr><td><H4>3.3 RMSprop algorithm</H4></td></tr>
	<tr><td>
		RMSprop (Root Mean Square propagation) improves Adagrad by replacing cumulative sum of squared gradients with an exponentially decaying average of squared gradients. The idea is to discard history from distant past and converge once in a convex region. <br><br>
	</td></tr>
	<tr><td><H5>3.3.1 Computing exponentially decaying average of squared gradients</H5></td></tr>
	<tr><td>
	For layer $i=L-1, L-2, ..., 1$,

	\begin{eqnarray}
	v_t^{(w,i)} &=& \rho v_{t-1}^{(w,i)} + (1 - \rho) \left[\frac{\partial C}{\partial w_t^{(i)}}\right]^2 \\
	v_t^{(b,i)} &=& \rho v_{t-1}^{(b,i)} + (1 - \rho) \left[\frac{\partial C}{\partial b_t^{(i)}}\right]^2 \\
	\end{eqnarray}

	where $v_0^{(w,i)}$ and $v_0^{(b,i)}$ both initialized to zero.<br><br>
	</td></tr>
	<tr><td><H5>3.3.2 Updating parameters</H5></td></tr>
	<tr><td>
	\begin{eqnarray}
	w_{t+1}^{(i)} &=& w_t^{(i)} - \frac{\eta_t}{\sqrt{{v}_t^{(w,i)} + \epsilon}} \left[\frac{\partial C}{\partial w_t^{(i)}}\right] \\
	b_{t+1}^{(i)} &=& b_t^{(i)} - \frac{\eta_t}{\sqrt{{v}_t^{(b,i)} + \epsilon}} \left[\frac{\partial C}{\partial b_t^{(i)}}\right]
	\end{eqnarray}

	Keras uses $\eta=0.001$, $\epsilon=10^{-6}$ and $\rho=0.9$ <br><br>
	</td></tr>
													      
	<tr><td><H4>3.6 Adam algorithm</H4></td></tr>
	<tr><td>
	Adaptive Moment (Adam) estimation considers both the running average of past gradients and the running average of past squared gradients. Let on iteration $t$ of updates, $m_t$ and $v_t$ denote, respectively the first moment (mean) and the second moment (uncentered variance) of gradients. 
	Let $\beta_1$ and $\beta_2$, with $0<\beta_1<1$ and $0<\beta_2<1$ both generally close to $1$, denote the decay rates for $m_t$ and $v_t$ respectively.<br><br>
	</td></tr>
	<tr><td><H5>References</H5></td></tr>
	<ul>
	     	<li> Diederik Kingma, Jimmy Ba, <a href=https://arxiv.org/abs/1412.6980v8 target=blank>Adam: A Method for Stochastic Optimization</a>
		<li> Sashank J. Reddi, Satyen Kale, Sanjiv Kumar, <a href=https://openreview.net/forum?id=ryQu7f-RZ target=blank>On the Convergence of Adam and Beyond</a>
	</ul>
	<tr><td><H5>3.6.1 Computation of the moments</H5></td></tr>
	<tr><td>
	At iteration $t\geq 0$ of updates, let 
	<ul>	
		<li> $m_t^{(w,i)}$ and $m_t^{(b,i)}$ denote $m_t$ with respect to $w_t^{(i)}$ and $b_t^{(i)}$, respectively
		<li> $v_t^{(w,i)}$ and $v_t^{(b,i)}$ denote $v_t$ with respect to $w_t^{(i)}$ and $b_t^{(i)}$, respectively. 
	</ul>

	\begin{eqnarray}
	m_t^{(w,i)} &=& \beta_1 m_{t-1}^{(w,i)} + (1 - \beta_1) \left[\frac{\partial C}{\partial w_t^{(i)}}\right] \\
	m_t^{(b,i)} &=& \beta_1 m_{t-1}^{(b,i)} + (1 - \beta_1) \left[\frac{\partial C}{\partial b_t^{(i)}}\right] \\
	v_t^{(w,i)} &=& \beta_2 v_{t-1}^{(w,i)} + (1 - \beta_2) \left[\frac{\partial C}{\partial w_t^{(i)}}\right]^2 \\
	v_t^{(b,i)} &=& \beta_2 v_{t-1}^{(b,i)} + (1 - \beta_2) \left[\frac{\partial C}{\partial b_t^{(i)}}\right]^2 \\
	\end{eqnarray}

	where $m_0^{(w,i)}$, $m_0^{(b,i)}$, $v_0^{(w,i)}$ and $v_0^{(b,i)}$ are all initialized to zero-vectors.<br><br>												      
	</td></tr>												      
  </table>
</td></tr></table>													     
</body>                                                                                                  
                                                                                                  
</html>
