
<html>
<head>
	<title> Welcome to the Homepage of Tanbir Ahmed! </title>
	<link rel="stylesheet" href="../style.css" type="text/css" />
        <link href='http://fonts.googleapis.com/css?family=Lobster+Two' rel='stylesheet' type='text/css'>
	 <link href='https://fonts.googleapis.com/css?family=Alegreya' rel='stylesheet'>
 	<link href='https://fonts.googleapis.com/css?family=Barlow Condensed' rel='stylesheet'>
  	<link href='https://fonts.googleapis.com/css?family=Roboto Condensed' rel='stylesheet'>
 	<link href='https://fonts.googleapis.com/css?family=Encode Sans Condensed' rel='stylesheet'>
        <link href='http://fonts.googleapis.com/css?family=Berkshire+Swash' rel='stylesheet' type='text/css'>
	<link href='https://fonts.googleapis.com/css?family=PT+Serif:400' rel='stylesheet' type='text/css'>
        <link href='https://fonts.googleapis.com/css?family=PT+Serif:400italic' rel=stylesheet' type='text/css'>
        <link href='http://fonts.googleapis.com/css?family=Merienda+One' rel='stylesheet' type='text/css'>
		<link href='http://fonts.googleapis.com/css?family=Simonetta' rel='stylesheet' type='text/css'>
        <meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
	<meta charset="UTF-8">
													      
	<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]}});</script>
													      
								
</head>

<body>
<table cellspacing=0 cellpadding=20 align=center><tr><td>													      
  <table cellspacing=0 cellpadding=0 align=center>
	<tr><td><H1>An implementation of a Feedforward Neural Network (FNN)</H1></td></tr>                                                                                                
	<tr><td><H2>Data Structure</H2></td></tr>												      
	<tr><td><H3>1. Layers and Parameters</H3></td></tr>                                                                                                
	<tr><td>
	<b>Layers:</b> Let $\ell_0, \ell_1, \ldots, \ell_L$ be the layers where <br>
	<ul>													      
		<li>	$\ell_0$ represents the input layer, 
    		<li>	$\ell_1, \ell_2, \ldots, \ell_{L-1}$ represent the hidden layers, and 
    		<li> 	$\ell_L$ represents the output layer. 
	</ul>												      
	Let $h_i$ denote the height of the layer $\ell_i$ for $i=0, 1, \ldots, L$ <br><br>													      
    	
	<b>Biases:</b> For $i=1, 2, \ldots, L$, let <br>
	<ul>
		<li> $b^{(i)} \in {\mathbb R}^{h_i}$ denote the bias for the layer $\ell_i$ and
		<li> $b^{(i)}_j \in {\mathbb R}$ denote the bias for the $j$-th neuron of $\ell_i$
	</ul>
	<b>Weights:</b> For $i=1, 2, \ldots, L$, let <br>
	<ul>
		<li> $w^{(i)} \in {\mathbb R}^{h_{i-1}\times h_{i}}$ denote the weight for the layer $\ell_i$ and
    		<li> $w^{(i)}_{jk}\in {\mathbb R}$ denote the weight from the $j$-th neuron of $\ell_{i-1}$ to the $k$-th neuron of $\ell_i$  													      
	</ul>
	<b>Layer outputs:</b> For $i=1, 2, \ldots, L$, $z^{(i)} \in {\mathbb R}^{h_i}$ for the layer $\ell_i$ be defined as follows:<br>    
    	<ul>
		<li>$z^{(1)} = \left(w^{(1)}\right)^T\cdot x + b^{(1)}$ where $x\in{\mathbb R}^{h_{i-1}}$ is the input vector
        	<li>$z^{(i)} = \left(w^{(i)}\right)^T\cdot a^{(i-1)} + b^{(i)}$ for $i=2,3,\ldots,L$ where $a^{(i)}$ is defined as below
	</ul>
	<b>Layer output activations:</b> For $i=0, 1, 2, \ldots, L$, let $a^{(i)}\in {\mathbb R}^{h_i}$ for the layer $\ell_i$ be defined as follows: <br>   
    	<ul>
		<li>$a^{(0)}\in {\mathbb R}^{h_0}$ is initialized to the input vector $x$ and  
        	<li>$a^{(i)}=\sigma(z^{(i)})$ for $i=1,2, \ldots,L$ with some activation function $\sigma$ (we discuss activation functions later)												      
	</ul>
	<b>Output of the network:</b> $a^{(L)} \in {\mathbb R}^{h_L}$ <br><br>												      
	</td></tr>												      
	<tr><td><H3>2. Computation of Errors</H3></td></tr>													      
	<tr><td>												      
	The most common optimization algorithms used in neural networks are different varients of Gradient Descent (GD) where the objective function is a cost function  CC  that we want to minimize.<br><br>
	<b>Cost function:</b> Let $C$ be some differentiable cost function (we discuss cost functions later)<br><br>
	<b>Error in output:</b> For $j\in\{1, 2, \ldots, h_L\}$ 

	    $$\begin{eqnarray*}
	    \delta^{(L)}_j &=& {\partial C}/{\partial z^{(L)}_j} \\
		 &=& \sum_k\left({\partial C}/{\partial a^{(L)}_k}\right) \left({\partial a^{(L)}_k}/{\partial z^{(L)}_j}\right)  \\ 
		 &=& \left({\partial C}/{\partial a^{(L)}_j}\right) \left({\partial a^{(L)}_j}/{\partial z^{(L)}_j}\right) 
	    \end{eqnarray*}$$
    
    	Hence, error in the output layer:
	    $$\begin{eqnarray*}
	    \delta^{(L)} &=& 
		  \begin{bmatrix}
		   {\partial C}/{\partial z^{(L)}_1} \\
		   {\partial C}/{\partial z^{(L)}_2} \\
		    \cdots \\
		   {\partial C}/{\partial z^{(L)}_{h_L}} 
		   \end{bmatrix} \\
		  &=&
			 \begin{bmatrix}
			 {\partial C}/{\partial a^{(L)}_1} \\
			{\partial C}/{\partial a^{(L)}_2} \\
			\cdots \\
			{\partial C}/{\partial a^{(L)}_{h_L}}
			\end{bmatrix}
			\odot 
			\begin{bmatrix}
			{\partial a^{(L)}_1}/{\partial z^{(L)}_1} \\
			{\partial a^{(L)}_2}/{\partial z^{(L)}_2} \\
			\cdots \\
			{\partial a^{(L)}_{h_L}}/{\partial z^{(L)}_{h_L}}
			\end{bmatrix}     \\           
		&=&  \nabla_a{C}\odot \sigma^{\prime}(z^{(L)})
	    \end{eqnarray*}$$													      
	
	<b>Error propagated to other layers:</b> For $i=L-1, L-2, \ldots, 2$,

	    $$\begin{eqnarray*}
	    \delta^{(i)}_j &=& {\partial C}/{\partial z^{(i)}_j} \\
		 &=& \sum_k\left({\partial C}/{\partial z^{(i+1)}_k}\right) \left({\partial z^{(i+1)}_k}/{\partial z^{(i)}_j}\right)  \\ 
		 &=& \left({\partial z^{(i+1)}_k}/{\partial z^{(i)}_j}\right) \delta^{(i+1)}_k 
	    \end{eqnarray*}$$
    
    		Since $z^{(i+1)} = \left(w^{(i+1)}\right)^Ta^{(i)}+b^{(i+1)}$, we have 
    
	    $$\begin{eqnarray}
		z^{(i+1)}_k &=& \sum_j\left(w^{(i+1)}\right)^T_{kj}a^{(i)}_j + b^{(i+1)}_k \\ 
		&=& \sum_jw^{(i+1)}_{jk}\sigma(z^{(i)}_j) + b^{(i+1)}_k     
		\end{eqnarray}$$
     
     		Therefore, ${\partial z^{(i+1)}_k}/{\partial z^{(i)}_j} = w^{(i+1)}_{jk}\sigma^\prime(z^{(i)}_j)$ which implies 
     		$$\delta^{(i)}_j = \sum_kw^{(i+1)}_{jk}\delta^{(i+1)}_k\sigma^\prime(z^{(i)}_j).$$ 
     
     	Hence,
    		$$\delta^{(i)} = \left(w^{(i+1)}\cdot \delta^{(i+1)}\right)\odot \sigma^{\prime}(z^{(i)}).$$													      
	</td></tr>											
  </table>
</td></tr></table>													     
</body>                                                                                                  
                                                                                                  
</html>
